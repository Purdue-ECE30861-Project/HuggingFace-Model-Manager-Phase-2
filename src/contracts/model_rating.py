import time

from pydantic import BaseModel, Field, model_validator
from typing import Any
from pathlib import Path
from multiprocessing import Pool
from .metric_std import MetricStd, T
from .artifact_contracts import Artifact, SizeScore
from ..backend_server.classes import available_datasets_and_code, bus_factor, code_quality, dataset_quality, license, performance_claims, ramp_up_time, size, threading


class Reproducibility(MetricStd[float]):
    metric_name="reproducibility"
    def calculate_metric_score(self, ingested_path: Path, artifact_data: Artifact, *args, **kwargs) -> float:
        return 0.5


class Reviewedness(MetricStd[float]):
    metric_name="reviewedness"
    def calculate_metric_score(self, ingested_path: Path, artifact_data: Artifact, *args, **kwargs) -> float:
        return 0.5

class TreeScore(MetricStd[float]):
    metric_name="tree_score"
    def calculate_metric_score(self, ingested_path: Path, artifact_data: Artifact, *args, **kwargs) -> float:
        return 0.5


class Size(MetricStd[SizeScore]):
    metric_name="size_score"
    def calculate_metric_score(self, ingested_path: Path, artifact_data: Artifact, *args, **kwargs) -> SizeScore:
        return SizeScore.test_value()


class ModelRating(BaseModel):
    """Model rating summary generated by the evaluation service."""
    name: str = Field(..., description="Human-friendly label for the evaluated model")
    category: str = Field(..., description="Model category assigned during evaluation")
    net_score: float = Field(..., description="Overall score synthesizing all metrics")
    net_score_latency: float = Field(..., description="Time (seconds) required to compute net_score")
    ramp_up_time: float = Field(..., description="Ease-of-adoption rating for the model", json_schema_extra={"calc":ramp_up_time.RampUpTime(metric_weight=0.1)})
    ramp_up_time_latency: float = Field(..., description="Time (seconds) required to compute ramp_up_time")
    bus_factor: float = Field(..., description="Team redundancy score for the upstream project", json_schema_extra={"calc":bus_factor.BusFactor(metric_weight=0.1)})
    bus_factor_latency: float = Field(..., description="Time (seconds) required to compute bus_factor")
    performance_claims: float = Field(..., description="Alignment between stated and observed performance", json_schema_extra={"calc":performance_claims.PerformanceClaims(metric_weight=0.1)})
    performance_claims_latency: float = Field(..., description="Time (seconds) required to compute performance_claims")
    license: float = Field(..., description="Licensing suitability score", json_schema_extra={"calc":license.License(metric_weight=0.1)})
    license_latency: float = Field(..., description="Time (seconds) required to compute license")
    dataset_and_code_score: float = Field(..., description="Availability and quality of accompanying datasets and code", json_schema_extra={"calc":available_datasets_and_code.AvailableDatasetAndCode(metric_weight=0.1)})
    dataset_and_code_score_latency: float = Field(...,
                                                  description="Time (seconds) required to compute dataset_and_code_score")
    dataset_quality: float = Field(..., description="Quality rating for associated datasets", json_schema_extra={"calc":dataset_quality.DatasetQuality(metric_weight=0.1)})
    dataset_quality_latency: float = Field(..., description="Time (seconds) required to compute dataset_quality")
    code_quality: float = Field(..., description="Quality rating for provided code artifacts", json_schema_extra={"calc":code_quality.CodeQuality(metric_weight=0.1)})
    code_quality_latency: float = Field(..., description="Time (seconds) required to compute code_quality")
    reproducibility: float = Field(..., description="Likelihood that reported results can be reproduced", json_schema_extra={"calc":Reproducibility(metric_weight=0.1)})
    reproducibility_latency: float = Field(..., description="Time (seconds) required to compute reproducibility")
    reviewedness: float = Field(..., description="Measure of peer or community review coverage", json_schema_extra={"calc":Reviewedness(metric_weight=0.1)})
    reviewedness_latency: float = Field(..., description="Time (seconds) required to compute reviewedness")
    tree_score: float = Field(..., description="Supply-chain health score for model dependencies", json_schema_extra={"calc":TreeScore(metric_weight=0.05)})
    tree_score_latency: float = Field(..., description="Time (seconds) required to compute tree_score")
    size_score: SizeScore = Field(..., description="Size suitability scores for common deployment targets", json_schema_extra={"calc":Size(metric_weight=0.05)})
    size_score_latency: float = Field(..., description="Time (seconds) required to compute size_score")

    @staticmethod
    def calculate_net_score(scores: dict[str, float]):
        net_score = 0.0

        for score in scores.values():
            net_score += score

        if net_score > 1.0 or net_score < 0.0:
            raise ValueError("Net score must be normalized to 0 and 1")

        return net_score

    @staticmethod
    def _run_metric(metric: MetricStd) -> tuple[str, float, Any, Any]:
        return metric.run_score_calculation()

    @staticmethod
    def generate_rating(ingested_path: Path, artifact: Artifact, processes: int) -> "ModelRating":
        metrics: list[MetricStd] = []
        for name, field in ModelRating.model_fields.items():
            if field.json_schema_extra and "calc" in field.json_schema_extra:
                metrics.append(field.json_schema_extra["calc"].set_params(ingested_path, artifact))

        start = time.time()

        scores: dict[str, Any] = dict()
        weighted_scores: dict[str, Any] = dict()
        latencies: dict[str, float] = dict()
        # for metric in metrics:
        #     name, latency, value = ModelRating._run_metric(metric)

        with Pool(processes=processes) as pool:
            result = pool.map(ModelRating._run_metric, metrics)
            for field_name, latency, score, weighted_score in result:
                scores[field_name] = score
                weighted_scores[field_name] = weighted_score
                latencies[f"{field_name}_latency"] = latency

        end = time.time()
        return ModelRating(
            name=artifact.metadata.name,
            category=artifact.metadata.type,
            net_score=ModelRating.calculate_net_score(weighted_scores),
            net_score_latency=end-start,
            **latencies,
            **scores
        )

    @staticmethod
    def test_value() -> "ModelRating":
        return ModelRating(
            name="Stirlitz",
            category="text-generation",
            net_score=0.85,
            net_score_latency=1.2,
            ramp_up_time=0.75,
            ramp_up_time_latency=0.5,
            bus_factor=0.65,
            bus_factor_latency=0.8,
            performance_claims=0.90,
            performance_claims_latency=1.5,
            license=1.0,
            license_latency=0.3,
            dataset_and_code_score=0.80,
            dataset_and_code_score_latency=1.0,
            dataset_quality=0.85,
            dataset_quality_latency=0.9,
            code_quality=0.75,
            code_quality_latency=0.7,
            reproducibility=0.70,
            reproducibility_latency=2.0,
            reviewedness=0.60,
            reviewedness_latency=0.6,
            tree_score=0.88,
            tree_score_latency=1.1,
            size_score=SizeScore.test_value(),
            size_score_latency=0.4
        )